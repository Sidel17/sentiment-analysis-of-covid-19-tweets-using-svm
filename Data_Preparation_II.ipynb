{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "## Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\butar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "train = pd.read_csv('D:/KULIAH/SEMESTER_7/PBA/PROYEK/sentiment-analysis-of-covid-19-tweets/Corona.csv')\n",
    "test = pd.read_csv('D:/KULIAH/SEMESTER_7/PBA/PROYEK/sentiment-analysis-of-covid-19-tweets/Corona_NLP_test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercasing \n",
    "train[\"OriginalTweet\"] = train[\"OriginalTweet\"].str.lower()\n",
    "test[\"OriginalTweet\"] = test[\"OriginalTweet\"].str.lower()\n",
    "\n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "train['TokenizedTweet']= train['OriginalTweet'].apply(word_tokenize_wrapper)\n",
    "test['TokenizedTweet']= test['OriginalTweet'].apply(word_tokenize_wrapper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\butar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords = set(stopwords)\n",
    "\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in stopwords]\n",
    "\n",
    "train['TokenizedTweet']= train['TokenizedTweet'].apply(stopwords_removal)\n",
    "test['TokenizedTweet']= test['TokenizedTweet'].apply(stopwords_removal)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer, LancasterStemmer, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advice talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist gp set up online shopping accounts if poss adequate supplies of regular meds but not over order\n"
     ]
    }
   ],
   "source": [
    "tokens_train = train['OriginalTweet'][1]\n",
    "print(tokens_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'd', 'v', 'i', 'c', 'e', ' ', 't', 'a', 'l', 'k', ' ', 't', 'o', ' ', 'y', 'o', 'u', 'r', ' ', 'n', 'e', 'i', 'g', 'h', 'b', 'o', 'u', 'r', 's', ' ', 'f', 'a', 'm', 'i', 'l', 'y', ' ', 't', 'o', ' ', 'e', 'x', 'c', 'h', 'a', 'n', 'g', 'e', ' ', 'p', 'h', 'o', 'n', 'e', ' ', 'n', 'u', 'm', 'b', 'e', 'r', 's', ' ', 'c', 'r', 'e', 'a', 't', 'e', ' ', 'c', 'o', 'n', 't', 'a', 'c', 't', ' ', 'l', 'i', 's', 't', ' ', 'w', 'i', 't', 'h', ' ', 'p', 'h', 'o', 'n', 'e', ' ', 'n', 'u', 'm', 'b', 'e', 'r', 's', ' ', 'o', 'f', ' ', 'n', 'e', 'i', 'g', 'h', 'b', 'o', 'u', 'r', 's', ' ', 's', 'c', 'h', 'o', 'o', 'l', 's', ' ', 'e', 'm', 'p', 'l', 'o', 'y', 'e', 'r', ' ', 'c', 'h', 'e', 'm', 'i', 's', 't', ' ', 'g', 'p', ' ', 's', 'e', 't', ' ', 'u', 'p', ' ', 'o', 'n', 'l', 'i', 'n', 'e', ' ', 's', 'h', 'o', 'p', 'p', 'i', 'n', 'g', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', 's', ' ', 'i', 'f', ' ', 'p', 'o', 's', 's', ' ', 'a', 'd', 'e', 'q', 'u', 'a', 't', 'e', ' ', 's', 'u', 'p', 'p', 'l', 'i', 'e', 's', ' ', 'o', 'f', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', ' ', 'm', 'e', 'd', 's', ' ', 'b', 'u', 't', ' ', 'n', 'o', 't', ' ', 'o', 'v', 'e', 'r', ' ', 'o', 'r', 'd', 'e', 'r']\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "pStems = [porter.stem(t) for t in tokens_train]\n",
    "print(pStems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'd', 'v', 'i', 'c', 'e', ' ', 't', 'a', 'l', 'k', ' ', 't', 'o', ' ', 'y', 'o', 'u', 'r', ' ', 'n', 'e', 'i', 'g', 'h', 'b', 'o', 'u', 'r', 's', ' ', 'f', 'a', 'm', 'i', 'l', 'y', ' ', 't', 'o', ' ', 'e', 'x', 'c', 'h', 'a', 'n', 'g', 'e', ' ', 'p', 'h', 'o', 'n', 'e', ' ', 'n', 'u', 'm', 'b', 'e', 'r', 's', ' ', 'c', 'r', 'e', 'a', 't', 'e', ' ', 'c', 'o', 'n', 't', 'a', 'c', 't', ' ', 'l', 'i', 's', 't', ' ', 'w', 'i', 't', 'h', ' ', 'p', 'h', 'o', 'n', 'e', ' ', 'n', 'u', 'm', 'b', 'e', 'r', 's', ' ', 'o', 'f', ' ', 'n', 'e', 'i', 'g', 'h', 'b', 'o', 'u', 'r', 's', ' ', 's', 'c', 'h', 'o', 'o', 'l', 's', ' ', 'e', 'm', 'p', 'l', 'o', 'y', 'e', 'r', ' ', 'c', 'h', 'e', 'm', 'i', 's', 't', ' ', 'g', 'p', ' ', 's', 'e', 't', ' ', 'u', 'p', ' ', 'o', 'n', 'l', 'i', 'n', 'e', ' ', 's', 'h', 'o', 'p', 'p', 'i', 'n', 'g', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', 's', ' ', 'i', 'f', ' ', 'p', 'o', 's', 's', ' ', 'a', 'd', 'e', 'q', 'u', 'a', 't', 'e', ' ', 's', 'u', 'p', 'p', 'l', 'i', 'e', 's', ' ', 'o', 'f', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', ' ', 'm', 'e', 'd', 's', ' ', 'b', 'u', 't', ' ', 'n', 'o', 't', ' ', 'o', 'v', 'e', 'r', ' ', 'o', 'r', 'd', 'e', 'r']\n"
     ]
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "lStems = [lancaster.stem(t) for t in tokens_train]\n",
    "print(lStems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'quality', 'couches', 'at', 'unbelievably', 'low', 'prices', 'available', 'to', 'order', '.', 'we', 'are', 'in', 'boksburg', 'gp', 'for', 'more', 'info', 'whatsapp', ':', '084', '764', '8086', '#', 'supertuesdsy', '#', 'powertalk', '#', 'covid_19', '#', 'sayentrepreneur', '#', 'djsbu', 'https', ':', '//t.co/hhdjhyq2dc']\n"
     ]
    }
   ],
   "source": [
    "tokens_test = word_tokenize(test['OriginalTweet'][10])\n",
    "print(tokens_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'qualiti', 'couch', 'at', 'unbeliev', 'low', 'price', 'avail', 'to', 'order', '.', 'we', 'are', 'in', 'boksburg', 'gp', 'for', 'more', 'info', 'whatsapp', ':', '084', '764', '8086', '#', 'supertuesdsi', '#', 'powertalk', '#', 'covid_19', '#', 'sayentrepreneur', '#', 'djsbu', 'http', ':', '//t.co/hhdjhyq2dc']\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "pStems = [porter.stem(t) for t in tokens_test]\n",
    "print(pStems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'qual', 'couch', 'at', 'unbeliev', 'low', 'pric', 'avail', 'to', 'ord', '.', 'we', 'ar', 'in', 'boksburg', 'gp', 'for', 'mor', 'info', 'whatsap', ':', '084', '764', '8086', '#', 'supertuesdsy', '#', 'powertalk', '#', 'covid_19', '#', 'sayentrepr', '#', 'djsbu', 'https', ':', '//t.co/hhdjhyq2dc']\n"
     ]
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "lStems = [lancaster.stem(t) for t in tokens_test]\n",
    "print(lStems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'hate', 'grocery', 'shopping', 'in', 'general', 'but', 'i', 'swear', 'i\\x92m', 'doing', 'it', 'online', 'next', 'shop', ',', 'can', 'not', 'deal', 'with', 'the', 'swathes', 'of', 'panic', 'buyers', 'at', 'all', '!', '#', 'covid', '?', '19', '#', 'coronavirus', '#', 'coronavirusuk', '#', 'anxiety', '#', 'panicbuyinguk', '#', 'morons']\n"
     ]
    }
   ],
   "source": [
    "tokens_train = word_tokenize(train['OriginalTweet'][100])\n",
    "print(tokens_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'hate', 'grocery', 'shopping', 'in', 'general', 'but', 'i', 'swear', 'i\\x92m', 'doing', 'it', 'online', 'next', 'shop', ',', 'can', 'not', 'deal', 'with', 'the', 'swathe', 'of', 'panic', 'buyer', 'at', 'all', '!', '#', 'covid', '?', '19', '#', 'coronavirus', '#', 'coronavirusuk', '#', 'anxiety', '#', 'panicbuyinguk', '#', 'moron']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(t) for t in tokens_train]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'qualiti', 'couch', 'at', 'unbeliev', 'low', 'price', 'avail', 'to', 'order', '.', 'we', 'are', 'in', 'boksburg', 'gp', 'for', 'more', 'info', 'whatsapp', ':', '084', '764', '8086', '#', 'supertuesdsi', '#', 'powertalk', '#', 'covid_19', '#', 'sayentrepreneur', '#', 'djsbu', 'http', ':', '//t.co/hhdjhyq2dc']\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "pStems = [porter.stem(t) for t in tokens_test]\n",
    "print(pStems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'qualiti', 'couch', 'at', 'unbeliev', 'low', 'price', 'avail', 'to', 'order', '.', 'we', 'are', 'in', 'boksburg', 'gp', 'for', 'more', 'info', 'whatsapp', ':', '084', '764', '8086', '#', 'supertuesdsi', '#', 'powertalk', '#', 'covid_19', '#', 'sayentrepreneur', '#', 'djsbu', 'http', ':', '//t.co/hhdjhyq2dc']\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "pStems = [porter.stem(t) for t in tokens_test]\n",
    "print(pStems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'quality', 'couch', 'at', 'unbelievably', 'low', 'price', 'available', 'to', 'order', '.', 'we', 'are', 'in', 'boksburg', 'gp', 'for', 'more', 'info', 'whatsapp', ':', '084', '764', '8086', '#', 'supertuesdsy', '#', 'powertalk', '#', 'covid_19', '#', 'sayentrepreneur', '#', 'djsbu', 'http', ':', '//t.co/hhdjhyq2dc']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(t) for t in tokens_test]\n",
    "print(lemmas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
